{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fd3402-5307-4545-8f2b-96afb6e73b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jjwhit/rcGAN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5754d92b-cfe6-4399-963d-4203541616ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjwhit/rcGAN/fastmri/__init__.py:16: UserWarning: Could not retrieve fastmri version!\n",
      "  warnings.warn(\"Could not retrieve fastmri version!\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import types\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from data.lightning.MassMappingDataModule import MMDataModule\n",
    "from utils.parse_args import create_arg_parser\n",
    "from pytorch_lightning import seed_everything\n",
    "from models.lightning.mmGAN import mmGAN\n",
    "from utils.mri.math import tensor_to_complex_np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6282f5-ac84-45c2-911c-0c61e7d42f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model here\n",
    "test_plot_model = mmGAN.load_from_checkpoint('/share/gpu0/jjwhit/mass_map/mm_models/mmgan_training_ks/checkpoint-epoch=88.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051b2b8-2780-495a-a988-af80927de730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e69351c-c1a7-4037-af0f-2381edba6c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 1\n",
      "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "VALIDATING EPOCH: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing generated distribution:   0%|          | 0/55 [00:00<?, ?it/s]/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "Computing generated distribution: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s]\n",
      "Computing generated distribution:   0%|          | 0/55 [00:00<?, ?it/s]/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "Computing generated distribution: 100%|██████████| 55/55 [00:22<00:00,  2.48it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [300, 300] at index 0 does not match the shape of the indexed tensor [495, 512] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/rcGAN/scripts/mass_map/validate.py:102\u001b[0m\n\u001b[1;32m     99\u001b[0m     best_epoch_cfid \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m    100\u001b[0m     best_cfid \u001b[38;5;241m=\u001b[39m cfid_val\n\u001b[0;32m--> 102\u001b[0m pearson_val \u001b[38;5;241m=\u001b[39m \u001b[43mpearsoncoeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m pearson_vals\u001b[38;5;241m.\u001b[39mappend((epoch, pearson_val))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pearson_val \u001b[38;5;241m>\u001b[39m best_pearson:\n",
      "File \u001b[0;32m~/rcGAN/mass_map_utils/scripts/ks_utils.py:137\u001b[0m, in \u001b[0;36mpearsoncoeff\u001b[0;34m(a, b, mask)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03margs:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    a (torch.float64): ground truth\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    pearson (float): Pearson correlation coefficient\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    138\u001b[0m     b \u001b[38;5;241m=\u001b[39m b[mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    139\u001b[0m a \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(a)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [300, 300] at index 0 does not match the shape of the indexed tensor [495, 512] at index 0"
     ]
    }
   ],
   "source": [
    "%run /home/jjwhit/rcGAN/scripts/mass_map/validate.py --config /home/jjwhit/rcGAN/configs/mass_map.yml --exp-name mmgan_training_ks --num-gpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8241006-e939-42a6-b8ea-ad0a8bb6718b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae012947-1dc8-4e5a-b272-c6f0a804bd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2955b4-8cb2-46fc-9f1a-ecea6a8eeffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8664928-725a-45b7-a6c1-3a02956e4504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c18f1-4285-49ee-9c81-3b3b54629cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run /home/jjwhit/rcGAN/scripts/mass_map/test.py --exp-name mmgan_training_8 --num-figs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a05c46-25a9-40a0-8862-786d96086a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ba34a-f091-4c9e-b076-6dafd025cdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e7976-ef57-456d-b449-5e48d65f85cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15f9ba-e560-4b33-9a06-74ad2f91dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13918633-383c-4434-bdbe-8b8a8489af91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92cb27-b494-4868-a65f-d049641cc8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b7dedc-a033-44d8-90ee-20f1a70138c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "import types\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "dir = '/home/jjwhit/rcGAN/'\n",
    "sys.path.append(dir)\n",
    "\n",
    "from data.lightning.MassMappingDataModule import MMDataModule\n",
    "from utils.parse_args import create_arg_parser\n",
    "from models.lightning.mmGAN import mmGAN\n",
    "from pytorch_lightning import seed_everything\n",
    "from utils.embeddings import VGG16Embedding\n",
    "from evaluation_scripts.mass_map_cfid.cfid_metric import CFIDMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11d5926-07aa-458f-a276-31df96ee8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2cb73a8-f961-4df3-bc98-ff529a3a72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(dct):\n",
    "    return types.SimpleNamespace(**dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbe128c-354f-4087-949f-81dfc2c80da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "# seed_everything(1, workers=True)\n",
    "\n",
    "with open(dir+'configs/mass_map.yml', 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    cfg = json.loads(json.dumps(cfg), object_hook=load_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6bf929-e2e0-4144-a2d0-8735c558368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dm = MMDataModule(cfg)\n",
    "dm.setup()\n",
    "val_loader = dm.val_dataloader()\n",
    "best_epoch = -1\n",
    "inception_embedding = VGG16Embedding()\n",
    "best_cfid = 10000000\n",
    "best_pearson = -1\n",
    "best_psnr = -1\n",
    "best_snr = -1\n",
    "best_rmse = 10000000\n",
    "start_epoch = 80  # Will start saving models after 80 epochs\n",
    "end_epoch = 100\n",
    "mask = np.load(\n",
    "    \"/home/jjwhit/rcGAN/mass_map_utils/cosmos/cosmos_mask.npy\", allow_pickle=True\n",
    ").astype(bool)\n",
    "\n",
    "cfid_vals = []\n",
    "psnr_vals = []\n",
    "snr_vals = []\n",
    "rmse_vals = []\n",
    "r_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac110024-0d5e-4476-b0d6-b44c837ad145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATING EPOCH: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing generated distribution:   0%|          | 0/55 [00:00<?, ?it/s]/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "/home/jjwhit/rcGAN/data/lightning/MassMappingDataModule.py:96: RuntimeWarning: divide by zero encountered in divide\n",
      "  F_kappa = F_gamma / D\n",
      "Computing generated distribution: 100%|██████████| 55/55 [00:22<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATING EPOCH: 81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALIDATING EPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmmGAN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/share/gpu0/jjwhit/mass_map/mm_models/mmgan_training_ks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/checkpoint-epoch=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:  \u001b[38;5;66;03m# type: ignore[valid-type]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:188\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:234\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 234\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "File \u001b[0;32m~/rcGAN/models/lightning/mmGAN.py:28\u001b[0m, in \u001b[0;36mmmGAN.__init__\u001b[0;34m(self, args, exp_name, num_gpus)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# self.in_chans = 6 # Re and im part of image; 2 dimensions of noise; Kaiser-Squires re and im - NOTE: This is temporary change, refactor when working\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_chans \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mout_chans\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[43mUNetModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_chans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in_chans=5, #NOTE: Temporary change here \u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_chans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator \u001b[38;5;241m=\u001b[39m DiscriminatorModel(\n\u001b[1;32m     35\u001b[0m     in_chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39min_chans \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mout_chans, \u001b[38;5;66;03m# Number of channels from x and y\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# in_chans=6, #NOTE: Temporary change here\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     out_chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_chans,\n\u001b[1;32m     38\u001b[0m     input_im_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mim_size\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_mult \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/rcGAN/models/archs/mass_map/generator.py:155\u001b[0m, in \u001b[0;36mUNetModel.__init__\u001b[0;34m(self, in_chans, out_chans, chans, num_pool_layers)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pool_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample_layers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mConvUpBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchans\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchans\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchans \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample_layers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ConvUpBlock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchans \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchans)]\n",
      "File \u001b[0;32m~/rcGAN/models/archs/mass_map/generator.py:89\u001b[0m, in \u001b[0;36mConvUpBlock.__init__\u001b[0;34m(self, in_chans, out_chans)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(in_chans \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mPReLU()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_chans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_chans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     90\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(out_chans),\n\u001b[1;32m     91\u001b[0m     nn\u001b[38;5;241m.\u001b[39mPReLU(),\n\u001b[1;32m     92\u001b[0m     ResidualBlock(out_chans),\n\u001b[1;32m     93\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py:450\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    449\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py:144\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py:150\u001b[0m, in \u001b[0;36m_ConvNd.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        print(f\"VALIDATING EPOCH: {epoch}\")\n",
    "        try:\n",
    "            model = mmGAN.load_from_checkpoint(checkpoint_path='/share/gpu0/jjwhit/mass_map/mm_models/mmgan_training_ks' + f'/checkpoint-epoch={epoch}.ckpt')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        if model.is_good_model == 0:\n",
    "            print(\"NO GOOD: SKIPPING...\")\n",
    "            continue\n",
    "\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "\n",
    "        cfid_metric = CFIDMetric(\n",
    "            gan=model,\n",
    "            loader=val_loader,\n",
    "            image_embedding=inception_embedding,\n",
    "            condition_embedding=inception_embedding,\n",
    "            cuda=True,\n",
    "            args=cfg,\n",
    "            ref_loader=False,\n",
    "            num_samps=1\n",
    "        )\n",
    "\n",
    "        recon, label, truth = cfid_metric._get_generated_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bec8f0-46bc-46fe-8d35-e9827cc9eb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd17d80-5925-46c9-b0d2-a332b6b3d7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc0b62-18f5-44cc-be84-26546f67dea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5c4e5-029b-4deb-81a0-2a83b3ec550a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc6fa6-40b2-4a98-a7a8-c1ac2a5244d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing CFID functions that the val script uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6ecd5b9-5187-498a-8aee-8eb0017a774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import types\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "dir = '/home/jjwhit/rcGAN/'\n",
    "sys.path.append(dir)\n",
    "from data.lightning.MassMappingDataModule import MMDataModule\n",
    "from utils.parse_args import create_arg_parser\n",
    "from models.lightning.mmGAN import mmGAN\n",
    "from pytorch_lightning import seed_everything\n",
    "from utils.embeddings import VGG16Embedding\n",
    "from evaluation_scripts.mass_map_cfid.cfid_metric import CFIDMetric\n",
    "# import torchvision.transforms as transforms\n",
    "from utils.mri.math import tensor_to_complex_np\n",
    "from utils.mri.transforms import unnormalize_complex\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6712d8d0-59c9-4102-9dc0-7b66b79677b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40765dd1-88c2-449a-b978-3b46cf1dfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(dct):\n",
    "    return types.SimpleNamespace(**dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c95bd14-513e-42cd-9407-02ef36623c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "# seed_everything(1, workers=True)\n",
    "\n",
    "with open(dir+'configs/mass_map_test.yml', 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    cfg = json.loads(json.dumps(cfg), object_hook=load_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f443ade4-4c28-4d48-9baa-39cac48e8e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dm = MMDataModule(cfg)\n",
    "dm.setup()\n",
    "val_loader = dm.val_dataloader()\n",
    "best_epoch = -1\n",
    "inception_embedding = VGG16Embedding()\n",
    "best_cfid = 10000000\n",
    "best_pearson = -1\n",
    "best_psnr = -1\n",
    "best_snr = -1\n",
    "best_rmse = 10000000\n",
    "start_epoch = 80  # Will start saving models after 80 epochs\n",
    "end_epoch = 100\n",
    "mask = np.load(\n",
    "    \"/home/jjwhit/rcGAN/mass_map_utils/cosmos/cosmos_mask.npy\", allow_pickle=True\n",
    ").astype(bool)\n",
    "\n",
    "cfid_vals = []\n",
    "psnr_vals = []\n",
    "snr_vals = []\n",
    "rmse_vals = []\n",
    "r_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "220e652d-03d1-474e-b8df-bcf3d58a6760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmGAN(\n",
       "  (generator): UNetModel(\n",
       "    (down_sample_layers): ModuleList(\n",
       "      (0): ConvDownBlock(\n",
       "        (conv_1): Conv2d(4, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): ConvDownBlock(\n",
       "        (conv_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): ConvDownBlock(\n",
       "        (conv_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (3): ConvDownBlock(\n",
       "        (conv_1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "    (res_layer_1): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): PReLU(num_parameters=1)\n",
       "      (3): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (5): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (6): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (up_sample_layers): ModuleList(\n",
       "      (0): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (discriminator): DiscriminatorModel(\n",
       "    (initial_layers): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): AvgPool(in_chans=32, out_chans=64\n",
       "      ResBlock(in_chans=64, out_chans=64\n",
       "      (1): AvgPool(in_chans=64, out_chans=128\n",
       "      ResBlock(in_chans=128, out_chans=128\n",
       "      (2): AvgPool(in_chans=128, out_chans=256\n",
       "      ResBlock(in_chans=256, out_chans=256\n",
       "      (3): AvgPool(in_chans=256, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "      (4): AvgPool(in_chans=512, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "      (5): AvgPool(in_chans=512, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "    )\n",
       "    (dense): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=8192, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mmGAN.load_from_checkpoint(checkpoint_path=cfg.checkpoint_dir + 'mmgan_training_3/checkpoint-epoch=88.ckpt')\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0086c795-ad7e-40fd-80b0-6d67b14f40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_generated_distribution(loader, gan, num_samps, args, cuda, ref_loader=None):\n",
    "    image_embed = []\n",
    "    cond_embed = []\n",
    "    true_embed = []\n",
    "    \n",
    "    # Replace self.loader, self.gan, self.num_samps, etc., with parameters passed to this function\n",
    "    for i, data in enumerate(loader):\n",
    "        condition, gt, mean, std = data\n",
    "        condition = condition.cuda()\n",
    "        gt = gt.cuda()\n",
    "        mean = mean.cuda()\n",
    "        std = std.cuda()\n",
    "        print('gt shape: ',gt.shape, 'condition shape: ',condition.shape, 'mean shape: ',mean.shape, 'std shape: ',std.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for l in range(num_samps):\n",
    "                recon = gan(condition)\n",
    "\n",
    "                image = _get_embed_im(recon, args.kappa_mean, args.kappa_std)\n",
    "                condition_im = _get_embed_im_complex(condition)\n",
    "                true_im = _get_embed_im(gt, args.kappa_mean, args.kappa_std)\n",
    "\n",
    "                img_e = image_embedding(transforms(image))\n",
    "                cond_e = condition_embedding(transforms(condition_im))\n",
    "                true_e = image_embedding(transforms(true_im))\n",
    "                print('img_e shape: ',img_e.shape, 'cond_e shape: ',cond_e.shape, 'true_e shape: ',true_e.shape)\n",
    "\n",
    "                if cuda:\n",
    "                    true_embed.append(true_e)\n",
    "                    image_embed.append(img_e)\n",
    "                    cond_embed.append(cond_e)\n",
    "                else:\n",
    "                    true_embed.append(true_e.cpu().numpy())\n",
    "                    image_embed.append(img_e.cpu().numpy())\n",
    "                    cond_embed.append(cond_e.cpu().numpy())\n",
    "\n",
    "    if ref_loader:\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(ref_loader):\n",
    "                condition, gt, mean, std = data\n",
    "                condition = condition.cuda()\n",
    "                gt = gt.cuda()\n",
    "                mean = mean.cuda()\n",
    "                std = std.cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for l in range(num_samps):\n",
    "                        recon = gan(condition)\n",
    "\n",
    "                        image = _get_embed_im(recon, args.kappa_mean, args.kappa_std)\n",
    "                        condition_im = _get_embed_im_complex(condition)\n",
    "                        true_im = _get_embed_im(gt, args.kappa_mean, args.kappa_std)\n",
    "\n",
    "                        img_e = image_embedding(transforms(image))\n",
    "                        cond_e = condition_embedding(transforms(condition_im))\n",
    "                        true_e = image_embedding(transforms(true_im))\n",
    "\n",
    "                        if cuda:\n",
    "                            true_embed.append(true_e)\n",
    "                            image_embed.append(img_e)\n",
    "                            cond_embed.append(cond_e)\n",
    "                        else:\n",
    "                            true_embed.append(true_e.cpu().numpy())\n",
    "                            image_embed.append(img_e.cpu().numpy())\n",
    "                            cond_embed.append(cond_e.cpu().numpy())\n",
    "                        print('REF LOADER: img_e shape: ',img_e.shape, 'cond_e shape: ',cond_e.shape, 'true_e shape: ',true_e.shape)\n",
    "\n",
    "    if cuda:\n",
    "        true_embed = torch.cat(true_embed, dim=0)\n",
    "        image_embed = torch.cat(image_embed, dim=0)\n",
    "        cond_embed = torch.cat(cond_embed, dim=0)\n",
    "    else:\n",
    "        true_embed = np.concatenate(true_embed, axis=0)\n",
    "        image_embed = np.concatenate(image_embed, axis=0)\n",
    "        cond_embed = np.concatenate(cond_embed, axis=0)\n",
    "    print('FINAL: image_embed shape: ',image_embed.shape, 'cond_embed shape: ',cond_embed.shape, 'true_embed shape: ',true_embed.shape)\n",
    "    return image_embed.to(dtype=torch.float64), cond_embed.to(dtype=torch.float64), true_embed.to(dtype=torch.float64)\n",
    "    \n",
    "def _get_embed_im(multi_coil_inp, mean, std, im_size=300):\n",
    "    # Initialize the tensor to store the embedded images\n",
    "    embed_ims = torch.zeros(size=(multi_coil_inp.size(0), 3, im_size, im_size)).cuda()\n",
    "    \n",
    "    for i in range(multi_coil_inp.size(0)):\n",
    "        # Create the reformatted tensor for each image in the batch\n",
    "        reformatted = torch.zeros(size=(1, im_size, im_size, 2)).cuda()\n",
    "        reformatted[:, :, :, 0] = multi_coil_inp[i, 0, :, :]\n",
    "        reformatted[:, :, :, 1] = multi_coil_inp[i, 1, :, :]\n",
    "\n",
    "        # Apply normalization based on mean and std\n",
    "        unnormal_im = reformatted * std + mean\n",
    "\n",
    "        # Convert to a complex numpy array and then to a real tensor\n",
    "        im = torch.real(torch.tensor(tensor_to_complex_np(unnormal_im.cpu()))).cuda()\n",
    "\n",
    "        # Normalize the image to the range [0, 1]\n",
    "        im = (im - torch.min(im)) / (torch.max(im) - torch.min(im))\n",
    "\n",
    "        # Assign the normalized image to the three channels of the output tensor\n",
    "        embed_ims[i, 0, :, :] = im\n",
    "        embed_ims[i, 1, :, :] = im\n",
    "        embed_ims[i, 2, :, :] = im\n",
    "\n",
    "    return embed_ims\n",
    "    \n",
    "def _get_embed_im_complex(multi_coil_inp, im_size=300):\n",
    "    # Initialize the tensor to store the embedded images\n",
    "    embed_ims = torch.zeros(size=(multi_coil_inp.size(0), 3, im_size, im_size)).cuda()\n",
    "\n",
    "    for i in range(multi_coil_inp.size(0)):\n",
    "        # Create the reformatted tensor for each image in the batch\n",
    "        reformatted = torch.zeros(size=(1, im_size, im_size, 2)).cuda()\n",
    "        reformatted[:, :, :, 0] = multi_coil_inp[i, 0, :, :]\n",
    "        reformatted[:, :, :, 1] = multi_coil_inp[i, 1, :, :]\n",
    "\n",
    "        # Unnormalize the complex image\n",
    "        unnormal_im = unnormalize_complex(reformatted)  # Mean/std calculated during preprocessing\n",
    "\n",
    "        # Convert to a complex numpy array and then to a real tensor\n",
    "        im = torch.real(torch.tensor(tensor_to_complex_np(unnormal_im.cpu()))).cuda()\n",
    "\n",
    "        # Normalize the image to the range [0, 1]\n",
    "        im = (im - torch.min(im)) / (torch.max(im) - torch.min(im))\n",
    "\n",
    "        # Assign the normalized image to the three channels of the output tensor\n",
    "        embed_ims[i, 0, :, :] = im\n",
    "        embed_ims[i, 1, :, :] = im\n",
    "        embed_ims[i, 2, :, :] = im\n",
    "\n",
    "    return embed_ims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c242140e-5259-46ed-93ad-b58615605978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_embedding = VGG16Embedding()\n",
    "condition_embedding = VGG16Embedding()\n",
    "transforms = T.Compose([\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d6d8191-49bb-4428-b825-c2e94ddc20b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "gt shape:  torch.Size([9, 2, 300, 300]) condition shape:  torch.Size([9, 2, 300, 300]) mean shape:  torch.Size([9]) std shape:  torch.Size([9])\n",
      "img_e shape:  torch.Size([9, 512]) cond_e shape:  torch.Size([9, 512]) true_e shape:  torch.Size([9, 512])\n",
      "FINAL: image_embed shape:  torch.Size([495, 512]) cond_embed shape:  torch.Size([495, 512]) true_embed shape:  torch.Size([495, 512])\n"
     ]
    }
   ],
   "source": [
    "recon, label, truth = _get_generated_distribution(val_loader, model, 1, cfg, True, ref_loader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5daa46-620e-4e02-8103-af70541381b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([495, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e6f071-44aa-4e5d-b27b-bde32fe05bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([495, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a39ab09-701d-4c25-b75c-e1e659f1964b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([495, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e131a6e-7520-4a3b-9b38-036bc3f8fb76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cGAN",
   "language": "python",
   "name": "cgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
